# import base64
# import json
# import re
# import requests
# import math
# from io import BytesIO
# import os
# import uuid
# import shutil

# from PIL import Image, ImageDraw

# # â–¼â–¼â–¼ [ìˆ˜ì •] AutoModelForVision2Seq ì¶”ê°€ â–¼â–¼â–¼
# import torch
# from transformers import AutoModelForVision2Seq, AutoTokenizer 
# from dotenv import load_dotenv
# from http import HTTPStatus
# try:
#     import dashscope
#     from dashscope import MultiModalConversation
#     _HAS_DASHSCOPE = True
# except ImportError:
#     _HAS_DASHSCOPE = False
# # â–²â–²â–² [ìˆ˜ì •] â–²â–²â–²

# # ... (prompt_ins ë“± ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë™ì¼) ...
# prompt_ins = '''Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and user will return the searched results. Every time you retrieve an image, you have the option to crop it to obtain a clearer view, the format for coordinates is <bbox>[x1, y1, x2, y2]</bbox>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: {question}
# '''

# class VRAG:
#     def __init__(self,
#                 planner_model_path='/root/workspace/VRAG_test/VRAG_lsm/grpo_model/30_step_checkpoint',
#                 search_url='http://0.0.0.0:8002/search',
#                 generator=True):
        
#         # ... (API ì„¤ì • ë¶€ë¶„ì€ ë™ì¼) ...
#         if not _HAS_DASHSCOPE:
#             raise ImportError("DashScope ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install \"dashscope[vl]\"'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
#         dotenv_path = '/root/workspace/VRAG_test/.env'
#         load_dotenv(dotenv_path=dotenv_path)
        
#         dashscope.base_http_api_url = os.getenv("EVAL_BASE_URL")
#         api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("EVAL_API_KEY")
        
#         if not api_key:
#             raise ValueError(f"'{dotenv_path}' íŒŒì¼ì— DASHSCOPE_API_KEY ë˜ëŠ” EVAL_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
#         dashscope.api_key = api_key
        
#         self.answerer_model_name = os.getenv("EVAL_MODEL", "qwen-vl-max")
#         print(f"âœ… 'ë‹µë³€ ëª¨ë¸'ë¡œ ì™¸ë¶€ API ({self.answerer_model_name})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

#         # 2. ê²€ìƒ‰ ê³„íš(Planner) ëª¨ë¸ ë¡œë“œ (ì§€ì •í•œ ë¡œì»¬ ëª¨ë¸)
#         print("ğŸ” ë¡œì»¬ 'ê²€ìƒ‰ ê³„íš ëª¨ë¸'ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
        
#         absolute_planner_path = os.path.abspath(planner_model_path)
#         print(f"ëª¨ë¸ì˜ ì ˆëŒ€ ê²½ë¡œ: {absolute_planner_path}")

#         self.planner_tokenizer = AutoTokenizer.from_pretrained(
#             absolute_planner_path,
#             trust_remote_code=True,
#             local_files_only=True
#         )

#         # â–¼â–¼â–¼ [ìˆ˜ì •] AutoModelForCausalLM -> AutoModelForVision2Seq ë¡œ ë³€ê²½ â–¼â–¼â–¼
#         self.planner_model = AutoModelForVision2Seq.from_pretrained(
#             absolute_planner_path,
#             torch_dtype=torch.bfloat16,
#             low_cpu_mem_usage=True,
#             trust_remote_code=True,
#             device_map='auto',
#             local_files_only=True
#         )
#         # â–²â–²â–² [ìˆ˜ì •] â–²â–²â–²
        
#         print("âœ… 'ê²€ìƒ‰ ê³„íš ëª¨ë¸' ë¡œë”© ì™„ë£Œ.")

#         self.search_url = search_url
#         self.max_pixels = 512 * 28 * 28
#         self.min_pixels = 256 * 28 * 28
#         self.repeated_nums = 1
#         self.max_steps = 10
#         self.generator = generator
#     # ... (process_image, search, _generate_plan í•¨ìˆ˜ ë“± ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•©ë‹ˆë‹¤) ...
#     def process_image(self, image):
#         if isinstance(image, dict):
#             image = Image.open(BytesIO(image['bytes']))
#         elif isinstance(image, str):
#             image = Image.open(image)

#         if (image.width * image.height) > self.max_pixels:
#             resize_factor = math.sqrt(self.max_pixels / (image.width * image.height))
#             width, height = int(image.width * resize_factor), int(image.height * resize_factor)
#             image = image.resize((width, height))

#         if (image.width * image.height) < self.min_pixels:
#             resize_factor = math.sqrt(min_pixels / (image.width * image.height))
#             width, height = int(image.width * resize_factor), int(image.height * resize_factor)
#             image = image.resize((width, height))

#         if image.mode != 'RGB':
#             image = image.convert('RGB')
        
#         byte_stream = BytesIO()
#         image.save(byte_stream, format="JPEG")
#         byte_array = byte_stream.getvalue()
#         base64_encoded_image = base64.b64encode(byte_array)
#         base64_string = base64_encoded_image.decode("utf-8")
#         base64_qwen = f"data:image;base64,{base64_string}"

#         return image, base64_qwen

#     def search(self,query):
#         if isinstance(query,str):
#             query = [query]
#         search_response = requests.get(self.search_url, params={"queries": query})
#         search_results = search_response.json()
#         image_path_list = [result['image_file'] for result in search_results[0]]
#         return image_path_list

#     def _generate_plan(self, messages):
#         query = self.planner_tokenizer.from_list_format(messages)
#         inputs = self.planner_tokenizer([query], return_tensors='pt').to(self.planner_model.device)
#         gen_kwargs = {"max_length": 2048, "do_sample": False} 
#         with torch.no_grad():
#             outputs = self.planner_model.generate(**inputs, **gen_kwargs)
#             response_text = self.planner_tokenizer.decode(outputs[0], skip_special_tokens=True)
#         last_response = response_text.split('<|im_start|>assistant\n')[-1]
#         return last_response.replace('<|im_end|>', '').strip()

#     def _generate_final_answer(self, original_question: str, collected_images: list):
#         print(f"âœï¸ ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ì™¸ë¶€ API '{self.answerer_model_name}'ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤...")

#         temp_dir = "temp_images_for_api"
#         os.makedirs(temp_dir, exist_ok=True)
#         image_paths = []
        
#         try:
#             for i, img in enumerate(collected_images):
#                 path = os.path.join(temp_dir, f"{uuid.uuid4().hex}.jpg")
#                 img.save(path)
#                 image_paths.append(path)
            
#             user_content = []
#             for path in image_paths:
#                 user_content.append({"image": "file://" + os.path.abspath(path)})
#             user_content.append({"text": original_question})

#             messages = [{
#                 "role": "user",
#                 "content": user_content
#             }]

#             response = MultiModalConversation.call(model=self.answerer_model_name, messages=messages)

#             if response.status_code == HTTPStatus.OK:
#                 content = response.output.choices[0].message.content[0]['text']
#                 raw_response = str(response)
#                 return 'answer', content.strip(), raw_response
#             else:
#                 error_msg = f"API Error: {response.code} - {response.message}"
#                 return 'answer', error_msg, str(response)

#         except Exception as e:
#             return 'answer', f"An exception occurred: {e}", ""
#         finally:
#             if os.path.exists(temp_dir):
#                 shutil.rmtree(temp_dir)
    
#     def run(self, question):
#         self.image_raw = []
#         self.image_input = []
#         self.image_path = []
#         prompt = prompt_ins.format(question=question)
#         messages = [dict(
#             role="user",
#             content=[
#                 {
#                     "type": "text",
#                     "text": prompt,
#                 }
#             ]
#         )]
        
#         max_steps = self.max_steps
#         while max_steps > 0:
#             response_content = self._generate_plan(messages)
#             messages.append(dict(
#                 role="assistant",
#                 content=[{ "type": "text", "text": response_content }]
#             ))
#             pattern = r'<think>(.*?)</think>'
#             match = re.search(pattern, response_content, re.DOTALL)
#             if not match:
#                 print("âš ï¸ <think> íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê³„íš ë‹¨ê³„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#                 break
#             thought = match.group(1)
#             if self.generator:
#                 yield 'think', thought, match.group(0)
#             pattern = r'<(search|bbox)>(.*?)</\1>'
#             match = re.search(pattern, response_content, re.DOTALL)
#             if not match:
#                 print("âœ… ê²€ìƒ‰/BBox í–‰ë™ì´ ì—†ì–´ ì •ë³´ ìˆ˜ì§‘ì„ ì™„ë£Œí•©ë‹ˆë‹¤.")
#                 break
#             raw_content = match.group(0)
#             content = match.group(2).strip()
#             action = match.group(1)
#             if self.generator:
#                 yield action, content, raw_content
            
#             user_content = []
#             if action == 'search':
#                 search_results = self.search(content)
#                 image_path = ""
#                 while len(search_results) > 0:
#                     temp_path = search_results.pop(0)
#                     if self.image_path.count(temp_path) < self.repeated_nums:
#                         self.image_path.append(temp_path)
#                         image_path = temp_path
#                         break
#                 if not image_path:
#                     user_content.append({"type": "text", "text": "Search returned no new images."})
#                 else:
#                     image_raw = Image.open(image_path)
#                     image_input, img_base64 = self.process_image(image_raw)
#                     user_content.append({ 'type': 'image_url', 'image_url': { 'url': img_base64 }})
#                     self.image_raw.append(image_raw)
#                     self.image_input.append(image_input)
#                     if self.generator:
#                         yield 'search_image', self.image_input[-1], raw_content
#             elif action == 'bbox':
#                 bbox = json.loads(content)
#                 input_w, input_h = self.image_input[-1].size
#                 raw_w, raw_h = self.image_raw[-1].size
#                 crop_region_bbox = bbox[0] * raw_w / input_w, bbox[1] * raw_h / input_h, bbox[2] * raw_w / input_w, bbox[3] * raw_h / input_h
#                 pad_size = 56
#                 crop_region_bbox = [max(crop_region_bbox[0]-pad_size,0), max(crop_region_bbox[1]-pad_size,0), min(crop_region_bbox[2]+pad_size,raw_h), min(crop_region_bbox[3]+pad_size,raw_h)]
#                 crop_region = self.image_raw[-1].crop(crop_region_bbox)
#                 image_input, img_base64 = self.process_image(crop_region)
#                 user_content.append({'type': 'image_url', 'image_url': { 'url': img_base64 }})
#                 self.image_raw.append(crop_region)
#                 self.image_input.append(image_input)
#                 if self.generator:
#                     image_to_draw = self.image_input[-2].copy()
#                     draw = ImageDraw.Draw(image_to_draw)
#                     draw.rectangle(bbox, outline=(160, 32, 240), width=7)
#                     yield 'crop_image', self.image_input[-1], image_to_draw
#             max_steps -= 1
#             messages.append(dict( role='user', content=user_content ))

#         action, content, raw_content = self._generate_final_answer(
#             original_question=question, 
#             collected_images=self.image_input
#         )
#         return action, content, raw_content

import base64
import json
import re
import requests
import math
from io import BytesIO
import os
import uuid
import shutil

from PIL import Image, ImageDraw

import torch
from transformers import AutoModelForVision2Seq, AutoTokenizer 
from dotenv import load_dotenv
from http import HTTPStatus
try:
    import dashscope
    from dashscope import MultiModalConversation
    _HAS_DASHOPE = True
except ImportError:
    _HAS_DASHOPE = False

prompt_ins = '''Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and user will return the searched results. Every time you retrieve an image, you have the option to crop it to obtain a clearer view, the format for coordinates is <bbox>[x1, y1, x2, y2]</bbox>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: {question}
'''

class VRAG:
    def __init__(self,
                planner_model_path='/root/workspace/VRAG_test/VRAG_lsm/grpo_model/30_step_checkpoint',
                search_url='http://0.0.0.0:8002/search',
                generator=True):
        
        # â¬…ï¸ [ë³€ê²½] __init__ì—ì„œ session_id ìë™ ìƒì„±ì„ ì œê±°í•˜ê³ , ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë§Œ ì„ ì–¸
        self.session_id = None
        self.request_idx = 0
        print(f"âœ… VRAG Agent Initialized. Session ID will be set at runtime.")
        
        if not _HAS_DASHOPE:
            raise ImportError("DashScope ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install \"dashscope[vl]\"'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        dotenv_path = '/root/workspace/VRAG_test/.env'
        load_dotenv(dotenv_path=dotenv_path)
        
        dashscope.base_http_api_url = os.getenv("EVAL_BASE_URL")
        api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("EVAL_API_KEY")
        
        if not api_key:
            raise ValueError(f"'{dotenv_path}' íŒŒì¼ì— DASHSCOPE_API_KEY ë˜ëŠ” EVAL_API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        dashscope.api_key = api_key
        
        self.answerer_model_name = os.getenv("EVAL_MODEL", "qwen-vl-max")
        print(f"âœ… 'ë‹µë³€ ëª¨ë¸'ë¡œ ì™¸ë¶€ API ({self.answerer_model_name})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        print("ğŸ” ë¡œì»¬ 'ê²€ìƒ‰ ê³„íš ëª¨ë¸'ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
        
        absolute_planner_path = os.path.abspath(planner_model_path)
        print(f"ëª¨ë¸ì˜ ì ˆëŒ€ ê²½ë¡œ: {absolute_planner_path}")

        self.planner_tokenizer = AutoTokenizer.from_pretrained(
            absolute_planner_path,
            trust_remote_code=True,
            local_files_only=True
        )

        self.planner_model = AutoModelForVision2Seq.from_pretrained(
            absolute_planner_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            device_map='auto',
            local_files_only=True
        )
        
        print("âœ… 'ê²€ìƒ‰ ê³„íš ëª¨ë¸' ë¡œë”© ì™„ë£Œ.")

        self.search_url = search_url
        self.max_pixels = 512 * 28 * 28
        self.min_pixels = 256 * 28 * 28
        self.repeated_nums = 1
        self.max_steps = 10
        self.generator = generator

    def process_image(self, image):
        # ... process_image í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼ ...
        if isinstance(image, dict):
            image = Image.open(BytesIO(image['bytes']))
        elif isinstance(image, str):
            image = Image.open(image)

        if (image.width * image.height) > self.max_pixels:
            resize_factor = math.sqrt(self.max_pixels / (image.width * image.height))
            width, height = int(image.width * resize_factor), int(image.height * resize_factor)
            image = image.resize((width, height))

        if (image.width * image.height) < self.min_pixels:
            resize_factor = math.sqrt(self.min_pixels / (image.width * image.height))
            width, height = int(image.width * resize_factor), int(image.height * resize_factor)
            image = image.resize((width, height))

        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        byte_stream = BytesIO()
        image.save(byte_stream, format="JPEG")
        byte_array = byte_stream.getvalue()
        base64_encoded_image = base64.b64encode(byte_array)
        base64_string = base64_encoded_image.decode("utf-8")
        base64_qwen = f"data:image;base64,{base64_string}"

        return image, base64_qwen

    def search(self, query: str):
        # â¬…ï¸ [ë³€ê²½] runì—ì„œ ì„¤ì •ëœ self.session_idì™€ self.request_idxë¥¼ ì‚¬ìš©
        request_body = [{
            "query": query,
            "id": self.session_id,
            "request_idx": self.request_idx
        }]
        
        print(f"â¡ï¸ Search Request Body: {json.dumps(request_body, indent=2)}")

        try:
            response = requests.post(self.search_url, json=request_body)
            response.raise_for_status()
            
            search_results_list = response.json()
            print(f"â¬…ï¸ Search Response Body: {json.dumps(search_results_list, indent=2)}")
            
            result_for_this_request = next((item for item in search_results_list if item.get("request_idx") == self.request_idx), None)
            
            if result_for_this_request:
                results = result_for_this_request.get("results", [])
                image_path_list = [res.get("image_file") for res in results if "image_file" in res]
                return image_path_list
            else:
                print(f"âš ï¸ Warning: ì‘ë‹µì—ì„œ request_idx {self.request_idx}ì— í•´ë‹¹í•˜ëŠ” ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []

        except requests.exceptions.RequestException as e:
            print(f"âŒ Error during search request: {e}")
            return []
        except json.JSONDecodeError:
            print("âŒ Error: ì„œë²„ ì‘ë‹µì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return []

    def _generate_plan(self, messages):
        # ... _generate_plan í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼ ...
        query = self.planner_tokenizer.from_list_format(messages)
        inputs = self.planner_tokenizer([query], return_tensors='pt').to(self.planner_model.device)
        gen_kwargs = {"max_length": 2048, "do_sample": False} 
        with torch.no_grad():
            outputs = self.planner_model.generate(**inputs, **gen_kwargs)
            response_text = self.planner_tokenizer.decode(outputs[0], skip_special_tokens=True)
        last_response = response_text.split('<|im_start|>assistant\n')[-1]
        return last_response.replace('<|im_end|>', '').strip()
    
    def _generate_final_answer(self, original_question: str, collected_images: list):
        # ... _generate_final_answer í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼ ...
        print(f"âœï¸ ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ì™¸ë¶€ API '{self.answerer_model_name}'ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤...")

        temp_dir = "temp_images_for_api"
        os.makedirs(temp_dir, exist_ok=True)
        image_paths = []
        
        try:
            for i, img in enumerate(collected_images):
                path = os.path.join(temp_dir, f"{uuid.uuid4().hex}.jpg")
                img.save(path)
                image_paths.append(path)
            
            user_content = []
            for path in image_paths:
                user_content.append({"image": "file://" + os.path.abspath(path)})
            user_content.append({"text": original_question})

            messages = [{
                "role": "user",
                "content": user_content
            }]

            response = MultiModalConversation.call(model=self.answerer_model_name, messages=messages)

            if response.status_code == HTTPStatus.OK:
                content = response.output.choices[0].message.content[0]['text']
                raw_response = str(response)
                return 'answer', content.strip(), raw_response
            else:
                error_msg = f"API Error: {response.code} - {response.message}"
                return 'answer', error_msg, str(response)

        except Exception as e:
            return 'answer', f"An exception occurred: {e}", ""
        finally:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)

    # â¬…ï¸ [ë³€ê²½] run ë©”ì†Œë“œ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
    def run(self, question, session_id: str, request_idx: int):
        # run ë©”ì†Œë“œê°€ ì‹œì‘ë  ë•Œ UIì—ì„œ ë°›ì€ ê°’ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ ì„¤ì •
        self.session_id = session_id
        self.request_idx = request_idx

        self.image_raw = []
        self.image_input = []
        self.image_path = []
        prompt = prompt_ins.format(question=question)
        messages = [dict(
            role="user",
            content=[
                {
                    "type": "text",
                    "text": prompt,
                }
            ]
        )]
        
        max_steps = self.max_steps
        while max_steps > 0:
            response_content = self._generate_plan(messages)
            # ... ì´í•˜ run ë©”ì†Œë“œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼ ...
            messages.append(dict(
                role="assistant",
                content=[{ "type": "text", "text": response_content }]
            ))
            pattern = r'<think>(.*?)</think>'
            match = re.search(pattern, response_content, re.DOTALL)
            if not match:
                print("âš ï¸ <think> íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê³„íš ë‹¨ê³„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            thought = match.group(1)
            if self.generator:
                yield 'think', thought, match.group(0)
            pattern = r'<(search|bbox)>(.*?)</\1>'
            match = re.search(pattern, response_content, re.DOTALL)
            if not match:
                print("âœ… ê²€ìƒ‰/BBox í–‰ë™ì´ ì—†ì–´ ì •ë³´ ìˆ˜ì§‘ì„ ì™„ë£Œí•©ë‹ˆë‹¤.")
                break
            raw_content = match.group(0)
            content = match.group(2).strip()
            action = match.group(1)
            if self.generator:
                yield action, content, raw_content
            
            user_content = []
            if action == 'search':
                search_results = self.search(content)
                image_path = ""
                while len(search_results) > 0:
                    temp_path = search_results.pop(0)
                    if self.image_path.count(temp_path) < self.repeated_nums:
                        self.image_path.append(temp_path)
                        image_path = temp_path
                        break
                if not image_path:
                    user_content.append({"type": "text", "text": "Search returned no new images."})
                else:
                    image_raw = Image.open(image_path)
                    image_input, img_base64 = self.process_image(image_raw)
                    user_content.append({ 'type': 'image_url', 'image_url': { 'url': img_base64 }})
                    self.image_raw.append(image_raw)
                    self.image_input.append(image_input)
                    if self.generator:
                        yield 'search_image', self.image_input[-1], raw_content
            elif action == 'bbox':
                bbox = json.loads(content)
                input_w, input_h = self.image_input[-1].size
                raw_w, raw_h = self.image_raw[-1].size
                crop_region_bbox = bbox[0] * raw_w / input_w, bbox[1] * raw_h / input_h, bbox[2] * raw_w / input_w, bbox[3] * raw_h / input_h
                pad_size = 56
                crop_region_bbox = [max(crop_region_bbox[0]-pad_size,0), max(crop_region_bbox[1]-pad_size,0), min(crop_region_bbox[2]+pad_size,raw_h), min(crop_region_bbox[3]+pad_size,raw_h)]
                crop_region = self.image_raw[-1].crop(crop_region_bbox)
                image_input, img_base64 = self.process_image(crop_region)
                user_content.append({'type': 'image_url', 'image_url': { 'url': img_base64 }})
                self.image_raw.append(crop_region)
                self.image_input.append(image_input)
                if self.generator:
                    image_to_draw = self.image_input[-2].copy()
                    draw = ImageDraw.Draw(image_to_draw)
                    draw.rectangle(bbox, outline=(160, 32, 240), width=7)
                    yield 'crop_image', self.image_input[-1], image_to_draw
            max_steps -= 1
            messages.append(dict( role='user', content=user_content ))

        action, content, raw_content = self._generate_final_answer(
            original_question=question, 
            collected_images=self.image_input
        )
        return action, content, raw_content